\chapter[Rep.\ and Gene Fam.]{Repeats and Gene Families}\label{chap:repeatsfamilies}

Although, repeats and gene families are both DNA sequences which appears multiple times in the genome, they differ in copy number, function and origin.
The repeat or gene family consist of set of sequences assigned to the family. The particular sequences may differ due to mutations acquired during the evolution. This may be substitution, insertion, or deletion of one or more nucleotides.

Repeats usually does not have any function and are often represented by several thousands of copies\cite{cell}. They are classified into multiple classes depending on their size and how they replicate.
\emph{Simple sequence repeate} consists of tandem arrays of up to thousands of copies of short sequences, ranging from 1 to 500 nucleotides.
\emph{Retrotransposons} are capable of moving to different sites in DNA by the reverse transcription.
\emph{DNA transposons} moves through the DNA by being copied and reinserted as DNA sequences\cite{cell}.

The gene families are groups of related genes.
Their copy number is usually much lower than in repeats, and they arise by duplication of an ancestral gene. The different members of the families diverge due to mutations during the evolution which may result in loss of ability to produce a functional gene product. The nonfunctional gene copies are called \emph{pseudogenes}.
Gene duplications can arise by two mechanisms: duplication of a segment of DNA, or by reverse transcription of an mRNA, followed by integration of the cDNA copy into a new chromosomal site.
Duplication by reverse transcription usually yields an inactive gene copy (\emph{processed pseudogene}).

As the repeats and gene families consists of multiple very similar sequences in DNA, they cause problems in DNA assembly and sequencing data analysis. NGS provides a large amount of short sequences from DNA (reads). Repetitive sequences are usually longer than reads, and therefore reads from various positions of the same repetitive DNA are very hard to distinguish, which makes hard to determine the size of the repetitive sequence.

Repetitive sequences forms a substantial part of DNA.\@ For example,
transposable elements forms more then 70\% of the genome in some plants and amphibians, and 45\% in human genome\cite{biemont2006genetics}.
Therefore studying repetitive sequences is one of the essential tasks in DNA analysis.

In this chapter we will present multiple approaches for detecting repetitive sequences and their copy numbers in several scenarios. We start by finding known repeats and gene families in assembled genome, and continue with de novo repeat finding in both assembled genome and reads. In the last section, we present a gene family evolution model.

\section[Known Repeats in Assem\dots]{Finding the Known Repeats and Gene Families in the Assembled Genome}

The known repeats and families are stored in several databases. Some examples of repeat databases include repbase\cite{repbase}, dfam\cite{dfam}, and one example of the gene family databases is pfam\cite{pfam}.

In the database search, we are usually interested in all  sequences from the database which have matching, homologous, subsequence in the query sequence. For example if we are querying the repeat database, we get a list of repeats from the database, which appears in our sequence.
There are two approaches for sequence finding. The first one is based on sequence similarity, which is computed by sequence alignment. The second one is based on profile hidden Markov models.

\paragraph{Searching by sequence similarity.}
In the local alignment, on the input there are two sequences and the output is alignment of some substrings of the sequences. Usually, we are interested in an alignment with highest score, or score above some threshold.
The standard algorithms for local alignment of two sequences look for the optimal local alignment according to some scoring scheme. They use a dynamic programming algorithm to find the optimal alignment. The time complexity of this approach is proportional to the product of the lengths of both sequences, which makes this approach very slow for long sequences.

Therefore there have been developed heuristic algorithms, which find alignments much faster, but may miss the best alignment in some circumstances. One of such algorithm is BLAST\cite{blast}.
BLAST firstly looks for exact matches of length $T$, called seeds. The lower the $T$ is, the lower is the probability of missing best alignment, but also the time of the computation increases.
Each seed is then extended to maximize the score. The standard BLAST algorithm searches only for gapless alignments (i.e.\ it does not consider any indels), but there are versions which also enables gaps in the alignment.

\paragraph{Searching by profile HMM.}
The \emph{hidden Markov model (HMM)}, is a generative probabilistic model, which generates a random sequence together with its annotation by the states. HMM is similar to the finite automaton. It has a finite number of states, transitions and emissions. At each step the HMM moves to the next state (which might be the same as the current) according to the transition probabilities, and emits a symbol (or nothing if it is a dummy state) according to the emission probabilities for the curent state.

The profile HMM have to take into account all mutations (substitutions, insertions and deletions) and return higher probability for the sequences belonging to the family, than for other sequences.
Assume, we have a multiple alignment of sequences from the family. The profile HMM\cite{profile-hmm} contains a sequence of $M$ states (match states), corresponding to matching positions in the alignment. Each state $m_k,\, 1 \leq k \leq M$, have a distinct emission distribution.
For each matching state $m_k$ there is a delete state $d_k$, which does not produce any nucelotide, and thus allows to skip any nucleotide. Finally there are $M+1$ insert states, which allows to insert any sequence before or after any nucleotide. For convinience, the dummy starting state $m_0$ and ending state $m_{M+1}$ are added.
The architecture of the model can be seen in figure~\ref{fig:profile-hmm}. The probability distributions are trained from the training sequences. See~\cite{profile-hmm} for more details.
\todo{obrazok}

Once a model is built it can be used for search. The search involves a local alignment of the profile HMM to the sequence.
There are various dynamic programming algorithms for performing the alignment. The standard algorithm for inferring in HMM is the Viterbi algorithm, which is very slow comparing to sequnece alignment method\cite{eddy2011accelerated}. Recently published algorithm, HMMER3\cite{eddy2011accelerated}, for the alignment uses multiple segment Viterbi algorithm\cite{eddy2011accelerated}, which enables fast computation of an optimal sum of multiple ungapped local
alignment segments. This can be used as heuristic filter before running the full algorithm.

There are various tools for finding repeats and gene families, implementing these methods.
The most used ones are repeat masker\cite{repeatmasker}, interproscan\cite{mitchell2015interpro}, HMMER\cite{eddy2011accelerated}.
The profile HMMs enable probabilistic approach to the sequence finding which has several advantages. It can contain more information about the family than a single consensus sequence used in sequence similarity approach, i.e.\ if at particular position $3/5$ of the sequences has A and $2/5$ of the sequences has T, the consensus sequence contains only information about A, whereas the profile HMM contains information about the distribution of nucleotides.

\section{De Novo Repeat Finding}

In practice not all repeats are known and contained in the databases and databases only for a few organisms was well constructed (e.g.\ repeat masker contains databases for human and mouse\cite{repeatmasker}). Therefore algorithms for finding new repeats and gene families are needed.

\subsection{Finding de Novo Repeats in the Assembled Genome}

With the beginning of NGS, sequencing become cheaper and genomes of more organisms were sequenced. The fully and correctly assembled genomes are best source for repeat analysis, thus the need for finding repeats in such sequences arised.

\subsubsection{Finding Tandem Repeats}
Tandem repeats are repeats which occurs consequently one after another. There are many tools for finding tandem repeats in the assembled genome, e.g.\ Tandem repeats finder\cite{trf}, ATR\cite{atr}, TANTAN\cite{tantan} and Sunflower model\cite{nanasi2014probabilistic}.

In this section we will briefly present the main idea used in tandem repeats finder.

The tandem repeats finder algorithm consists of two components: detection component, which detects possible tandem repeat candidates, and analysis component, which analyses the candidates to filter actual tandem repeats.

The detection algorithm looks for exactly matching \kmers\ which are separated by $d$ nucleotides ($d$ is not specified in advance). It keeps a list of all possible \kmers\ (probes).
For each probe $p$ it stores a history list $H_p$.
When a particular position $i$ is added to the $H_p$, for all previous positions $j$ in $H_p$, $d = i - j$ becomes a possible pattern size.
The algorithm also maintains a distance list $D_d$, which is basically a sliding window of length $d$ and tracks the positions of all matches and their total. The distance list is updated every time a match at distance $d$ is detected. The updates sets right end of the window to $i$ and matches before $j = i - d$ are dropped from $D_d$ and subtracted from the total. The information in $D_d$ is then tested using statistical tests and if it passes, it is passed to analysis component of the algorithm.

In analysis, the pattern consisting of positions $j+1\dots i$ is selected and aligned with the surrounding sequences using wraparound dynamic programming\cite{fischetti1992apostolico, myers1989approximate}. If at least two copies are aligned, the tandem repeat is reported.

The more details on the statistical criteria and indel handling can be found in~\cite{trf}. There are also other approaches to tandem repeat finding, e.g.\ based on Hidden Markov models (\cite{tantan, nanasi2014probabilistic}).

Next, we will focus mainly on general repeats, which copies in contrast to the tandem repeats appears on random positions in the genome.

\subsubsection{Finding General Repeats}

The first methods (e.g.~\cite{reputer, repeatfinder, recon, repeatgluer, piler}) for finding repeats in assembled genome were based on computation of pairwise similarities. However, this approach has several disadvantages. In large genomes, which contain big repeat families, the computation of pairwise repeats would be very slow. For example the \textit{Alu} repeat family has over 1 million occurences in human genome which leads to $~10^{12}$ pairwise alignments.
Also, local sequence alignments do not usually correspond to the biological boundaries.\cite{recon}.

In~\cite{repscout}, they present an efficient method of similarity search which enables a rigorous definition of repeat boundaries. The main idea is to firstly find seeds formed from highly abundant \kmers\ and then extend it to the repeat family consensus.

If we have a seed for a particular family, we can use a simple greedy algorithm to first extend it to the right and then to the left. The algorithm extends a seed one base at a time, adding a base which has a majority among all sequences. It discards a sequences which stop align to the consensus and stops when most sequences stops to align. More precisely, let $S1,\dots S_n$ be strings containing occurrences of a repeat family, which share a \kmer\ seed, the consensus sequence $Q$ is the sequence which maximizes
$$A(Q; S_1 \dots S_n) = \left[\sum_k a(Q, S_k)\right] - c |Q|,$$
where $a(Q, S_k)$ is a fit-preferred\cite{repscout} alignment score and $c$ is a repeat frequency threshold.
Algorithm then starts from $Q = \text{\kmer\ seed}$ and greedily extends $Q$ with the nucleotide which maximizes the score, until it fails to improve the score in predefined number of consecutive iterations.

Since many abundant \kmers\ may arise from a single repeat family, to avoid multiple processing of the same family we remove all processed \kmers\ from the \kmer\ count table.

Finding tandem repeats is easier task than general repeat finding and it can complement the general repeat finding methods, i.e.\ the tandem repeats may be masked first from the genome so the general method has to find less repeats, which may lead to improved performance.

\subsection{Finding Repeats in Reads}\label{sect:repeats-reads}

The methods described in previous subsection needed assembled genome, which is not always available. Next we will present several methods for de novo repeat finding in the sequencing reads. The key property of the sequencing data is that sequences in the genome with higher abundance, will have higher representation in the sequencing data. Thus these methods has to identify such sequences.

\subsubsection{Finding Repeats by K-mer Abundance Histogram Analysis}

Some of the algorithms for the genome size estimation described in Section~\ref{sec:kmerhist} can be also used for detecting repeats.
In the basic form the algorithms tell us, the proportion of the repetitive sequences and their repeat count. In~\cite{waterman}, they also provided an algorithm for finding the consensus for repeats.

The algorithm uses two basic observations:

\begin{enumerate}
  \item The repeat family is expected to have multiple \kmers\ with  unusually high coverage.
  \item The reads covering a different copies of a particular repeat family can be separated into different groups, starting from one read and extending it if there are more then a prespecified number of reads, which prefix matches the suffix of the currently considered read.
\end{enumerate}

Using (1), we can find parts of repetitive sequences and using (2), we can extend them to obtain a consensus of a repeat family.

We can also obtain a copy number of the repeat family. In~\cite{waterman}, they described two methods for finding the copy number of the family.
The first is to calculate the sum of lengths of all reads similar to the consensus and dividing it by the estimated coverage.
The second way is to firstly find the reads which (a) suffix matches the prefix of the consensus, (b) are similar to some substring in consensus, (c) prefix matches the suffix of the consensus, and then build a directed graph from those reads, in which reads are vertices and edges are from read $i$ to read $j$ if the suffix of $i$ matches prefix of $j$ and the overlap is greater than some threshold. Finally we find all paths from reads from category (a) to category (c).

The major disadvantage of this method is that it needs error free reads and a higher coverage.
Next we present two methods which works also in low coverage scenario.

\subsubsection{Partial Sequence Assembly}

As we mentioned in Section~\ref{sec:kmerhist}, we can look at the NGS sequencing as a random sampling process. We select a random position in the genome and read $N$ bases starting that position, where $N$ is the length of a read. We do this many times depending on the target coverage.
Repetitive sequences have multiple copies in a genome, so we expect more occurrences of repetitive sequences than single-copy sequences in the sequencing data. If the coverage is low, there will be very few single copy sequences in the data and the chance that two of them are overlapping is very low. On the other side the average coverage of repetitive sequences will be higher and thus the chance of overlapping repetitive sequences will be higher.

The \emph{partial sequence assembly} method works as follows: As an input we take low coverage NGS sequencing data. Using assembly algorithms, we assemble the data into contigs. We take into account only contigs, which consists of at least $m$ reads. The appropriate value of $m$ can be computed from the coverage. The number of contigs, $M$, expected containing a number of reads $j$ is given by equation\cite{swaminathan2007global}:
$$E(M) = Ne^{-2c\sigma}{(1-e^{c\sigma})}^{j-1}$$
$$\sigma = 1 - \frac{T}{L},$$
where $c$ is the coverage, $L$ is the read length, and $T$ the base pair overlap required for contig formation.

The estimated copy number, $C$, within any sequence window was then calculated by\cite{swaminathan2007global}:
$$C = \frac{o}{e}$$
$$e = \frac{cw}{L},$$
where $o$ represents the observed number of reads matching in the sequence window, $e$ represents the expected number of reads matching a single copy sequence window of size $w$, $c$ is the coverage, and $L$ is the read length.

This method allows simple repeat analysis of low coverage sequencing data, which is especially useful in analysis of large genomes e.g.\ plant genomes. This method was successfully used for repeat analysis of soybean (\textit{Glycine max}) from $0.07\times$ coverage sequencing data\cite{swaminathan2007global}.
However, this method suffers from several drawbacks. The disadvantage of this method is that larger repeats can be split into multiple sequences. In addition, it depends on slow assembly process.

\subsubsection{The Sequencing Data Clustering}

The other approach to the de novo repeat detection and analysis is \emph{sequencing data clustering}. The goal is to separate sequences into multiple groups, where each group consists of similar sequences.
The key data structure for such clustering is a graph, where reads are vertices, and edges are between similar sequences. The edges can be labeled by the similarity score. Such graph can be constructed by performing all-to-all pairwise comparisons and recording all read pairs with sequence overlaps exceeding a specified treshold\cite{pertea2003tigr, novak2010graph}. The pairwise alignments can be effectively computed using mgblast\cite{pertea2003tigr}, which can filter out alignments with similarity below some threshold.

The basic method of clustering the graph, used in \emph{tclust}\cite{pertea2003tigr}, is to split it by connected components. The granularity of such graph strongly depends on sequence coverage. The higher the coverage is, the smaller number and bigger clusters will be found in the graph. Additionally, this methods may merge multiple independent repeats into one cluster. This is caused by so called bridge reads, which are similar to multiple repetitive sequences.

The issues of the basic method can be fixed by more clever clustering algorithm --- \emph{hierarchical aglomeration}. The aim of hierarchical aglomeration is to split the graph into clusters (communities) which are more connected inside as outside the component. The metrics used for evaluating of the property is \emph{modularity}.
The modularity $Q$ denotes the frequency of edges within a community in respect to the expected number of edges in a random graph. It can be masured as\cite{novak2010graph}:
$$Q = \frac{1}{2m}\sum\left[A_{ij}-\frac{k_i k_j}{2m}\right] \delta(c_i c_j),$$
where $k_i,\, k_j$ are the degrees of the vertices $i,\,j$, $m$ is the overall number of edges in the graph, $A_{ij}$ is one if $i$ and $j$ are neighbors and zero otherwise, and $\delta(c_i, c_j)$ is one if community $c_i = c_j$.

We can find the clustering with maximal modularity using a greedy algorithm. In the beginning every vertex has its own community.
At each iteration, for every pair of communities, the expected gain to the modularity if they are merged, $\Delta Q$, is computed. The communities with the greatest $\Delta Q$ are then merged. This is repeated until only one community is left. Finally, the iteration with maximal overall $Q$ is selected as final clustering.
\todo{figures}

In this section, we presented three methods for the de novo repeat finding in the sequencing reads. The first method is quick, but works only on higher coverage. The second and the last methods are focused on low coverage scenario. Using the hierarchical clustering algorithm gives a clusters which are in size somewhere between the partial sequence method, which tends to separate the same repeat family into multiple clusters, and the basic tclust method, which often merges unrelated repeat families into same cluster.

\section{Evolution of Gene Families}

Prior the wide availability of whole genome assemblies, allowed by newer sequencing technologies, evolution studies focused on small numbers of nucleotide differences between orthologous genes. Nowadays, it is possible to focus on large-scale genome differences, e.g.\ changes in the sizes of gene families.

The gene families may very among different organisms. In the evolutionary biology, we would like to distinguish which changes are due to evolutionary forces and which are just by chance. To do this, we need some null model, which models ``normal'' (random) evolution of the gene families. One such model is a birth-death based model presented in~\cite{hahn2005estimating}.

Suppose, that in a gene family the number of genes is given by discrete random variable $X(t)$. The probability that $X(t) = c$, given that $X(0) = s$ is $P(X(t) = c | X(0) = s)$.
In \emph{birth-death model}, we define two parameters: birth rate $\lambda$ and death rate $\mu$, where the probability that any gene being duplicated (and fixed) is $\lambda \Delta t$ or being lost is $\mu \Delta t$. In family of size $X(t)$ the possible transitions are\cite{hahn2005estimating}:
\begin{itemize}
  \item probability of one gain: $\lambda X(t) \Delta t + o(\Delta t)$
  \item probability of one loss: $\mu X(t) \Delta t + o(\Delta t)$
  \item probability of more then one event: $o(\Delta t)$
  \item probability of no change: $1 - (\lambda + \mu) X(t) \Delta t + o(\Delta t)$
\end{itemize}
If a gene family contains zero genes, it is not possible to gain or lose any genes, therefore it is considered as absorbing state.

We often set $\lambda = \mu$. In that case, the transition probabilities are\cite{hahn2005estimating}:
$$P(X(t) = c | X(0) = s) = \sum_{j=0}^{\min(s, c)} {s \choose j}{s+x-j-1 \choose s-1}\alpha^{s+c-2j}{(1-2\alpha)}^j,$$
where $\alpha = \frac{\lambda t}{1+ \lambda t}$ and $s \geq 1$. The mean and variance are\cite{hahn2005estimating}:
\begin{align*}
  \operatorname{Mean}(X(t) = c | X(0) = s) &= s\\
  \operatorname{Var}(X(t) = c | X(0) = s) &= 2s\lambda t
\end{align*}

Based on the birth-death model and the structure of \emph{phylogenetic tree}, we can construct \emph{probabilistic graph model}, which parametrizes the probability distributions of the gene family sizes in the tree.

Phylogenetic trees are heavily used in the study of the evolution. The leaves of a phylogenetic tree are the studied species and internal nodes are their common ancestors. In our case we will be using rooted binary phylogenetic tree (Figure~\ref{}\todo{obrazok}), where root is the common ancestor of all studied species and at every speciation event (internal node) only two species arise. The length of edges corresponds to the evolution time between a node and its parent. Usually only information about leaves is available and we need to estimate the information about ancestral organisms.

The birth-death model represents the distributions on the edges of the phylogenetic tree. Since only family sizes in the leaves are known, we are interested only in marginal probabilities of leaf nodes. The marginal probabilities can be computed by averaging over all posible assignments of the internal nodes (except the root). This can be done efficiently using dynamic programing algorithms\cite{felsenstein1981evolutionary}. In this case, we will use a common $\lambda$ for all branches in the tree.

To test the hypothesis against null model, the $P$-value is usually used. The $P$-value is defined as the probability of obtaining a result with equal to or lower likelihood than what was actually observed.

In order to compute the likelihood and its corresponding value, we need to specify a prior distribution for the root node. The prior should be noninformative and a natural choice is a uniform distribution\cite{felsenstein1981evolutionary}. However, this prior introduces an undesirable bias --- it attributes a larger likelihoods to smaller gene families\cite{hahn2005estimating}. Other priors also showed to be unsuitable\cite{hahn2005estimating}.

Since the root family size is not known, we need to use conditional $P$-values conditioned of specific value of the root family size.
The conditional $P$-value is computed as the probability that a random gene family with the same root size has smaller conditional likelihood.
To perform a statistic test, we can use supremum $P$-value, which is the maximal conditional $P$-value computed from reasonable range of root family size.
If such supremum $P$-value is small, it means that it is unlikely explainable by the birth-death model.

If we detect unlikely gene family evolution, we might want to know, which branch of the phylogenetic tree is responsible for the violation of the model.
One method of identification is to try to delete every branch (one at a time) and recompute the $P$-values. If the $P$-value increases largely, it means that the other branches followed the birth-dead model and thus this branch is responsible for the low $P$-value.
The other method is to enable each branch to have an independent $\lambda$. The parameters can be computed by expectation maximisation algorithm and the likelihood is then compared with the likelihood of model constrained to have a common $\lambda$ for all branches.

The approach presented in this section has several limitations. It assumes that rates in the whole phylogenetic tree is the same, which might not be true in some organisms. Moreover the unlikely branch detection assumes that only one branch is responsible for the different behavior of family sizes evolution.

\section{Conclusion}

In this chapter we presented several methods for repeat finding in various scenarios.
Some of this methods, mostly those which find repeats from reads (Section~\ref{sect:repeats-reads}) are developed for the reads which has significantly higher abundance than the gene families, and may need to be adjusted to be used for gene family discovery.

In last section we presented one model for gene family evolution. The presented method worked with exact counts of the gene families. If we have counted the gene family sizes from reads, only approximate counts (e.g.\ given by some probability distributions) would be available. The model needs to be modified to work with probabilities instead of exact counts in order to work with such counts.
We will address this issues in our dissertation thesis, see Section~\ref{sect:repeats-families}.
