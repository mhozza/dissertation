% chktex-file 1
\chapter{K-mer Counting and K-mer histograms}

% intor o kmeroch a podobne

Counting of the $k$-mer (substring of length~$k$) occurrences is one of the key problem in many applications in Bioinformatics.
It is used in error correction algorithms, DNA assemblers, sequence aligners and repeat detectors. It is also suitable for various sequence read analysis and genome size estimation. In many cases, computation of the $k$-mer abundance spectra (or $k$-mer histogram) is sufficient. The $k$-mer histogram is an array, where $i$-th element corresponds to the number of unique $k$-mers occurring in the data exactly $i$ times.

In the NGS data, there are usually a lot of reads, containing many errors producing a large amount of unique $k$-mers.
This causes a high memory usage of naive approach to this problem.
\todo{mozno nejaky priklad s cislami}

There are several existing tools addressing this issue for $k$-mer counting or $k$-mer histogram computation available, for example tallymer\cite{tallymer}, jellyfish\cite{jellyfish}, bfcounter\cite{bfcounter}, dsk\cite{dsk} and khmer\cite{khmer}. Each of the tool must handle some trade-off between speed, memory usage, disk usage, precision and random access capability.

There are multiple approaches to the efficient $k$-mer counting problem. Most of them involve hash tables, bloom filters or suffix arrays.

\section{Naive solution}

Let us first briefly explain the naive solution, so we can extend and compare it to the methods used in previously mentioned tools.

Suppose we want to count $k$-mers in sequence $S$.
We create a hash table $T$, and then go through the sequence $S$.
At each position $i$, we take the $k$-mer $K_i$ corresponding to that  position, compute $hash(K_i)$ and increase the corresponding item\footnote{Suppose we have some collision resolution algorithm.} in the table.

We can then query the hash table for the number of occurrences of particular $k$-mer, or for the histogram computation iterate through it to get all the counts and increment the corresponding element in the histogram array.

\section{Computing K-mer Histogram using Enhanced Suffix Arrays}

An \firstUseOf{Enhanced Suffix Arrays} are Suffix Arrays enhanced with some additional information --- in this case with \firstUseOf{lcp} (longest common prefix) information.
In contrary to the Suffix Trees they use less memory and are faster due to poor locality of memory reference of the Suffix Trees, causing efficiency loss on cached processor architectures.
Every algorithm using a suffix tree can be replaced with an equivalent
algorithm (with same time complexity) based on a suffix array and additional information\cite{enhancedsuffixarrays}.

We can represent Enhanced Suffix Array of string $S$ as two tables:
\begin{enumerate}
  \item \emph{suftab} --- array of integers in the range $0\dots n$, where $n = |S|$, specifying the lexicographic ordering of the $n + 1$ suffixes of $S\$ $.
  \item \emph{lcptab} --- array of integers in the range $0\dots n$, where lcp-value $lcptab[i]$ is the length of the longest common prefix of $S_{subtab[i]}$ and $S_{subtab[i-1]}$, for $1 \leq i \leq n$ and $lcptab[0] = 0$.
\end{enumerate}

We also need to introduce the concept of the \firstUseOf{lcp-intervals} and \firstUseOf{lcp-interval trees}.
\begin{definition}
An interval $[i..j]$, $0 \leq i < j \leq n$, is an lcp-interval of lcp-value $\ell$ if

\begin{enumerate}
\item $lcptab[i] < \ell$,
\item $lcptab[k] \geq l$ for all $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[k] = \ell$ for at least one $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[j + 1] < \ell$.
\end{enumerate}
\cite{enhancedsuffixarrays}
\end{definition}

Next we define parent-child relationships between lcp-intervals and thus an lcp-interval tree.

\begin{definition}
  An $m$-interval $[l..r]$ is said to be \emph{embedded} in an $\ell$-interval $[i..j]$ if it is a~subinterval of $[i..j]$ (i.e., $i \leq l < r \leq j $) and $m > \ell$.\footnote{Note that we cannot have both $i = l$ and $r = j$ because $m > \ell$.}
  The $\ell$-interval $[i..j ]$ is then called the interval \emph{enclosing} $[l..r]$. If $[i..j]$ encloses $[l..r]$ and there is no interval embedded in $[i..j]$ that also encloses $[l..r]$, then $[l..r]$ is called a child interval of $[i..j]$.\cite{enhancedsuffixarrays}
\end{definition}

The lcp-interval tree is a conceptual (or virtual) tree (i.e.\ we don't need to construct it explicitly in any of the algorithms). The root of the tree is the $0$-interval $[0..n]$.

The lcp-interval has an important property to the $k$-mer counting problem. An interval $[i..j]$ represents a string occurring $j - i + 1$ times in S\cite{tallymer}. To count the $k$-mer occurrence counts, we need to read the occurrence counts from the lcp-interval tree. We can use the algorithm from~\cite{enhancedsuffixarrays} to enumerate the nodes. The algorithm has following important features:

\begin{enumerate}
  \item The nodes are enumerated in bottom-up order.
  \item The children of the particular node are enumerated in lexicographic order.
  \item Whenever we process the children of a node, we have access to the lcp-value of the parent node % toto je skopcene, chceme to prepisat?
  \item The values in suftab and lcptab are accesed sequentially from left to right.
\end{enumerate}

We can compute the $k$-mer histogram $H_k$ using the algorithm for enumeration lcp-interval tree nodes. We start with processing the singleton intervals $[i..i]$. The singleton interval $[i..i]$ corresponds to the suffix $S_{suftab[i]}$. Let $d$ be the lcp-value of the parent node of the interval $[i..i]$, $S[suftab[i]..suftab[i]+k-1]$ is a $k$-mer occurring exactly once in $S$ if and only if $d < k$ and $suftab[i]+k < n$. We increment $H_k[1]$ by one for each interval satisfying the former conditions.
Then we process the other $\ell$-intervals $[i..j]$, (except the root node). Again, let $d$ be the lcp-value of the parent of interval $[i..j]$, $S[suftab[j]..suftab[j] + k - 1]$ is a $k$-mer occurring exactly $j - i + 1$ times in $S$ if and only if $d < k \leq \ell$ and $suftab[i]+k < n$. We increment $H_k[j-i+1]$ by one for each interval satisfying the former conditions.

One of the advantages of this approach, is that we can compute $H_k$ for different $k$ at once --- at each step we increment specific positions in all corresponding histograms $H_k$ for each $k$ we are interested.

To address the problem of counting $k$-mers in multiple sequences (reads), we can concatenate all reads to one sequence, using dividers $\$_1\dots \$_r$. To compute occurrences for large sequences, we need to split the sequence into multiple shorter non-overlaping sequences and use divide and conquer algorithm described in\cite{tallymer}.

\section{Filtering Unique K-mers using Bloom Filter}

In NGS data, there are a lot of errors in the reads. Error rate in NGS data is usually about 1--5\%. This means that probability of 20-mer containing error with error rate 1\% is $1 - 0.99^{20} \approxeq 0.18$ i.e.\ about 18\% of $k$-mers from same position in DNA will be different. Together with low probability of $k$-mers from different location be the same and low probability that errors cause $k$-mers from different positions to be same, we get most $k$-mers unique in low to mid coverage data. If we do not store these $k$-mers in hash table, we can save a lot of memory. We can use more memory efficient, probabilistic data structure called \firstUseOf{Bloom Filter}\cite{bloomfilter} to store unique $k$-mers.

The Bloom filter supports two operations --- \method{add} for adding an element to the Bloom filter and \method{test} for testing whether an element is in Bloom filter. The following property holds for the \method{test} operation:
\begin{itemize}
  \item If the element is set in the Bloom filter, returns \emph{true}.
  \item If the element is not set it returns \emph{false} with high probability.
\end{itemize}
If the test method returns true, the element is probably set in the filter, otherwise it is definitely not there.

The Bloom Filter basically consists of a bit vector of length $n$ and $m$ hashing functions, each transforming the element to the position in the bit vector. The parameters directly affect the false positive rate of the Bloom filter.
The \method{add} operation computes $m$ positions from $m$ hashing functions applied to the element and sets them to one.
The \method{test} operation computes $m$ positions from the element and checks whether each of them is one. If any of them is zero, returns false, otherwise returns true. Further details are described in~\cite{bloomfilter}.

We can use Bloom Filter to store information about all $k$-mers we have seen at some point of computation. We just add every $k$-mer to the Bloom filter during the computation. Before adding the $k$-mer, we first check if the $k$-mer is already in the Bloom filter. If yes, it means we (probably) have seen this $k$-mer before and we also add it to the hash table. This way we get approximate counts for each $k$-mer.
With one more pass we can get exact counts. We go through the $k$-mers once again and check if they are not in hash table. If not, they are certainly unique. More details are in~\cite{bfcounter}.

\section{Probabilistic K-mer Counting Using Count-Min Sketch}

The Count-Min sketch\cite{countminsketch} data structure enables counting of objects in sublinear space. It supports two operations:
\begin{itemize}
  \item \method{incrementcount(x)} %chktex 36
  \item \method{getcount(x)} %chktex 36
\end{itemize}

The data structure consists of $m$ hash tables, with different hash functions.
The \method{incrementcount(x)} operation computes hash of $x$ using each hash function and increments corresponding counters in all hash tables. The \method{getcount} operation computes hash of $x$ using each hash function and takes the minimum of all corresponding values in hash tables as the result. %chktex 36

If there is a collision in each table for a particular object, the \method{getcount} operation may return greater value for such object. This false positive rate depends on the collision probability, which is proportional to the number of hash tables and their sizes.

This data structure can be directly used to the $k$-mer counting problem. As it does not store the $k$-mers itself, it allows in memory $k$-mer counting and querying, which is useful for streaming applications.
More details about false positive rate estimation and comparison to other approaches can be found in~\cite{khmer}.

\section{Other Improvements}

There are two more improvements of $k$-mer counting methods we have found in the papers about $k$-mer counting.

\paragraph{} The first is memory usage improvement using explicit disk-memory trade-off. The \emph{Disk Streaming of K-mers (DSK)} algorithm\cite{dsk} exploits the fact that the multi-set of $k$-mers can be partitioned. Each partition can be loaded in the memory separately.

The algorithm uses a simplifying assumption: Let $d$ be the total number of distinct $k$-mers in the input; we assume that the number of distinct $k$-mers having a given hash value is at most $\lceil d/H \rceil$, where $H$ is the size of the hash table\cite{dsk}.

The algorithm works in $n_{iters} = \lceil vb/D\rceil$ iterations, where $v$ is total number of $k$-mers, $b$ is number of bits used to store the $k$-mer ($b = 2^{\lceil\log_2(2k)\rceil}$). Each $k$-mer is written to the disk only in $(h(m) \mod n_{iters})$-th iteration.

At each iteration algorithm maintains $n_p = \lceil\frac{vb+32}{0.7n_{iters}M}\rceil$ lists of $k$-mers.
It goes through all the $k$-mers in the input and writes the $k$-mers valid for the iteration to the $h(m)/n_{iters} \mod n_p$-th list on the disk.
Then for each list on the disk it constructs in-memory hash table storing the counts for each $k$-mer in the disk.
Finally it outputs every key-value pair in the hash table.

This algorithm is proved to use $D$ bits of disk space and $M$ bits of memory\cite{dsk}.

\paragraph{} The second is speed improvement by parallelism. To enable parallelism, we need to allow concurrent access to the key data structure for $k$-mer counting --- hash table, which must be shared between threads. The threads have to be able to read from and write to hash table at once, without locking the access to it. Otherwise we would loose the advantage of parallel execution, because other threads would need to wait the lock to be released. The thread safe, lock-free hash table implementation was presented in this~\cite{jellyfish}. It uses \emph{compare-and-swap} (CAS) instruction which is widely available in the modern processors. The CAS instruction does these operations atomically:
\begin{lstlisting}
def cas(location, oldvalue, newvalue):
  currentvalue = read(location)
  if currentvalue == oldvalue:
    set(location, newvalue)
  return currentvalue
\end{lstlisting}
If two threads access same memory location simultaneously, one of them will fail to set new value. In this case, the thread must take appropriate action.
It can retry the CAS operation until it is successful.

The algorithm for $k$-mer counting hash table works in two steps:
First it computes the location of the $k$-mer in the hash table and then it increments the value. Both of the operations can be done thread-safe without locks using the CAS instruction.

\section{Conclusion}

We briefly summarized the most important approaches to the \kmer counting problem. We also described the key data structures and optimizations for the \kmer counting --- namely enhanced suffix arrays, lock free hash tables, bloom filters and count-min sketch.

There are a lot of available tools for \kmer counting varying in their approach to the problem and in their functionality. Tallymer uses suffix arrays, Jellyfish uses parallel approach, BFCounter uses bloom filter to filter out the unique \kmers from the hash table, Khmer uses count-min sketch as it's data structure and DSK uses the partitioning approach.

The tools have various limitations --- for example DSK does not provide random access to the counts and Khmer does not count \kmers exactly.

The comparison of the tools can be found in~\cite{khmer}.
% cosi o pouziti a pod
