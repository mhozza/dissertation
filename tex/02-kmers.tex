\chapter[K-mer Counting]{K-mer Counting and K-mer Abundance Histograms}

A $K$-mer is a substring of length exactly~$k$. In \emph{$k$-mer counting} we take sequencing reads as an input and we want to know for each $k$-mer occurring in the reads, how many occurrences does it have. We can summarize the counts of all $k$-mers, in a $k$-mer histogram.
The \emph{$k$-mer abundance histogram $H_k$ (shortly the $k$-mer histogram)} is an array, where the $i$-th element (denoted by $H_k[i]$) is the number of unique $k$-mers occurring in the data exactly $i$ times. The size of the histogram denoted by $|H_k|$ is maximal $i$ such that $H_k[i] > 0$.

Counting of the $k$-mer occurrences is one of the key problem in many applications in bioinformatics.
For example, it was used to separate several individual
genomes when sequencing a metagenomic mixture of different microbes isolated from a certain environment~\cite{Wu2011, Wang2012}.
It is also used in error correction algorithms e.g.\ for correcting errors prior the DNA assembly~\cite{Pevzner2001}, sequence aligners~\cite{edgar2004muscle} and repeat detectors~\cite{caponnetto2013efficiency}.
It is also suitable for various sequence read analyses and genome size estimation~\cite{covest, williams, waterman}.
For some of these applications, computation of the $k$-mer abundance histogram is sufficient.
In particular, we will use the $k$-mer histograms in~Chapter~\ref{chap:genomesize} for estimating the coverage and the size of a genome.

A typical NGS data set, consists of many reads. Although these reads typically cover the target genome multiple times, sequencing errors in the reads result in many unique $k$-mers.
This causes a high memory usage of naive approaches to the problem of $k$-mer counting or $k$-mer histogram computation.
For example the 100Mb long genome with no repeats, sequenced with $50\times$ coverage, read length 100, and error rate $1\%$ contains approximately $4\cdot 10^9$ unique 21-mers. To just store a counter for each of them, we need approximately $40$ GB of memory. For larger genomes (e.g.\ 3.2Gb long human genome, or 16Gb onion (Allium cepa) genome), much more memory would be needed.

There are several existing tools addressing this issue, for example Tallymer~\cite{tallymer}, JELLYFISH~\cite{jellyfish}, BFCounter~\cite{bfcounter}, DSK~\cite{dsk} and Khmer~\cite{khmer}. Each tool must handle the trade-off between speed, memory usage, disk usage, precision and random access capability. Most approaches to the $k$-mer counting problem use hash tables, Bloom filters or suffix arrays.

\section{Simple Hashing Approach}

Let us first briefly explain the naive solution, so that we can extend and compare it to the methods used in selected published tools.
Suppose we want to count occurrences of $k$-mers in sequence $S$.
We create a hash table $T$, in which $k$-mers will be keys and their counts stored as values, and then scan the sequence $S$.
At each position $i$, we take the $k$-mer $K_i$ corresponding to that  position, compute $hash(K_i)$ and increase the corresponding item in the table, after handling possible collisions.

We can then query the hash table for the number of occurrences of a~particular $k$-mer, or iterate through the whole table to compute the histogram.

For small genomes with very low sequencing error rate, this approach is sufficient. However, higher error rate produces more unique $k$-mers and together with large genomes makes the hash table so big it does not fit in computer's memory. In addition, the general hash table is not parallelizable, which makes this approach slow.

\section[Filtering Unique K-mers]{Filtering Unique K-mers Using Bloom Filters}

Error rate in Illumina sequencing machines is usually about 1--5\%. However even at $1\%$, the probability that a 20-mer contains an error is $1 - 0.99^{20} \approxeq 0.18$; i.e.\ about 18\% of $k$-mers from a given position in DNA will differ from the correct $k$-mer.
The correct $k$-mers abundances are distributed according to the Poisson distribution with mean $c$, where $c$ is an average number of reads covering a single position in a genome (see Chapter~\ref{chap:genomesize} for more details). Thus they provide only $N_{cor}/c$ different $k$-mers.
Since both, the probability of $k$-mers from different location be the same and the probability that errors cause $k$-mers from different positions to be same are negligible. We can compute a proportion of the erroneous $k$-mers among all different $k$-mers as $$\alpha = \frac{N_{err}}{N_{err} + N_{cor}},$$
where $N_{cor}$ and $N_{err}$ is number of correct $k$-mers and erroneous $k$-mers respectively.

The higher the coverage, the higher the proportion of erroneous $k$-mers among all different $k$-mers. But at the same time the probability, that multiple errors lead to the same resulting $k$-mer increases. The lower the coverage, the higher the proportion of unique $k$-mers among all correct $k$-mers. Therefore, we get most $k$-mers unique in low to middle coverage data.
If we do not store these unique $k$-mers in the hash table, we can save a lot of memory. We can use a more memory efficient, probabilistic data structure called \emph{Bloom filter}~\cite{bloomfilter} to store the unique $k$-mers.

The Bloom filter stores a set of elements (in our case $k$-mers), and it supports two operations --- \method{add} for adding an element to the set and \method{test} for testing whether an element is in the set. The following property holds for the \method{test} operation:
\begin{itemize}
  \item If the element is in the set, test returns \emph{true}.
  \item If the element is not in the set, test returns \emph{false} with a high probability.
\end{itemize}
If the test method returns true, the element is probably in the set, otherwise it is definitely not there.

A Bloom filter consists of a bit vector of length $n$ and $m$ hashing functions, each transforming the element to the position in the bit vector. The parameters $n$ and $m$ directly affect the false positive rate of the Bloom filter.
The \method{add} operation computes $m$ positions from $m$ hashing functions applied to the element and sets all corresponding bits in the vector to one.
The \method{test} operation computes $m$ positions from the element and checks whether each of them is set to one. If any of them is zero, it returns false, otherwise it returns true. Further details are described in~\cite{bloomfilter}.

We can use a Bloom filter to store all $k$-mers we have seen at some point of computation. We just add every $k$-mer to the Bloom filter during the computation. Before adding the $k$-mer, we first check if the $k$-mer is already in the Bloom filter. If yes, it means that we have probably seen this $k$-mer before, and we also add it to the hash table with the starting count 2. In this way we get approximate counts for each $k$-mer.
With one more pass through the input data we can get exact counts.
Any input $k$-mer, which is not in the hash table is certainly unique, and the counts of $k$-mers in the hash table can be also recomputed. This approach was used in BFCounter~\cite{bfcounter}.

\section[Probabilistic K-mer Counting]{Probabilistic K-mer Counting Using Count-Min Sketch}

The Count-Min sketch~\cite{countminsketch} data structure enables counting objects in sublinear space. It supports two operations:
\begin{itemize}
  \item \method{incrementcount(x)} %chktex 36
  \item \method{getcount(x)} %chktex 36
\end{itemize}

The data structure consists of $m$ hash tables with different hash functions.
The \method{incrementcount(x)} operation computes hash of $x$ using each hash function and increments corresponding counters in all hash tables. The \method{getcount} operation computes hash of $x$ using each hash function and takes the minimum of all corresponding values in hash tables as the result. %chktex 36

If there is a collision in each table for a particular object, the \method{getcount} operation may return greater value for such object. This false positive rate depends on the collision probability, which decreases with increasing number of hash tables and their sizes.

This data structure can be directly used to the $k$-mer counting problem. As it does not store the $k$-mers themselves, it allows in memory $k$-mer counting and querying, which is useful for streaming applications.
More details about false positive rate estimation and comparison to other approaches for $k$-mer counting can be found in~\cite{khmer}.

\section[Other Improvements]{Other Improvements of the Hashing Approach}

The basic hashing approach to $k$-mer counting can be further improved in various ways; here we describe two such improvements.

\paragraph{} The first is memory usage improvement using explicit disk-memory trade-off. The \emph{Disk Streaming of K-mers (DSK)} algorithm~\cite{dsk} exploits the fact that the multi-set of $k$-mers can be partitioned, and each partition can be loaded to the memory separately.

The algorithm has two parameters: the memory size $M$ and disk space $D$ expressed in bits. During the computation, the algorithm does not exceed the memory and disk space constraints given by the parameters.

The algorithm works in several iterations, which number is proportional to the memory needed to store all the $k$-mers, and inversely proportional to the $D$, and maintains a multiple $k$-mer lists on the disk.
Each $k$-mer is written to the disk only once, at the iteration determined by the hashing function.
At each iteration, all the $k$-mers valid for the iteration are partitioned into the $k$-mer lists according to the hashing function.
Then for each list on the disk it constructs in-memory hash table storing the counts for each $k$-mer in the disk.
Finally it outputs every key-value pair in the hash table.

\paragraph{} The second improvement is the use of parallelism. To enable parallelism, we need to allow concurrent access to the key data structure for $k$-mer counting --- a hash table, which must be shared between threads. The threads have to be able to read from and write to the hash table simultaneously, without locking the access to it. Otherwise we would loose the advantage of parallel execution, because other threads would need to wait the lock to be released. A thread safe, lock-free hash table implementation was used in software jellyfish~\cite{jellyfish}. It uses a \emph{compare-and-swap} (CAS) instruction which is widely available in the modern processors. The CAS instruction does these operations atomically:
\begin{lstlisting}
def cas(location, oldvalue, newvalue):
  currentvalue = read(location)
  if currentvalue == oldvalue:
    set(location, newvalue)
  return currentvalue
\end{lstlisting}
If two threads access same memory location simultaneously, one of them will fail to set new value. In this case, the thread must take an appropriate action.
It can retry the CAS operation until it is successful.

The algorithm for $k$-mer counting hash table works in two steps:
First it computes the location of the $k$-mer in the hash table and then it increments the value. Both of the operations can be done in a thread-safe way without locks using the CAS instruction.

\section[Enhanced Suffix Arrays]{Computing K-mer Histogram Using Enhanced Suffix Arrays}

\emph{Suffix tree}~\cite{suffixtree} is a data structure widely used in string processing. Basically, it is a compressed lexicographical tree build from all suffixes of a string. Let's have an alphabet $\Sigma$ and a string $S \in \Sigma^*$. Usually, for easier construction and manipulation, suffix tree is constructed from string $S\$$ instead of $S$, where $\$$ is a lexicographically smaller than every character in $\Sigma$. The suffix tree can be constructed in $O(|S|)$ time and space complexity~\cite{suffixtree}.

\emph{Suffix arrays}~\cite{suffixarray} were developed as more space efficient replacement of suffix trees. The suffix array requires only $4n$ bytes\footnote{We suppose, that we can use 32bit integer values to store the data.} in its basic form, whereas a suffix tree require $20n$ bytes in worst case~\cite{kurtz1999reducing}. The suffix array is an array containing indices of lexicographically sorted suffixes of $S\$$ (similarly to suffix arrays we append $\$$ end marker to the original string $S$). Similarly to the suffix tree, a suffix array can be constructed in linear time~\cite{karkkainen2003simple, kim2003linear, ko2003space}. The suffix arrays are also faster due to poor locality of memory reference of the Suffix Trees, causing efficiency loss on cached processor architectures.

With basic suffix arrays it was not possible to directly replace the suffix tree with the suffix array in any suffix tree based string processing problem and maintain its time complexity. For example, using only the basic suffix array, it takes $O(m \log n)$ time in the worst case to answer decision queries of the type ``Is P a substring of S?'', where $m = |P|$.

\emph{Enhanced Suffix Arrays} are Suffix Arrays enhanced with some additional information --- in this case with the \emph{lcp (longest common prefix)} information.
In contrast to the Suffix Trees, enhanced suffix arrays use less memory (the lcp table can be stored in additional $4n$ bytes) and they can replace suffix trees in every algorithm maintaining the same time complexity~\cite{enhancedsuffixarrays}.

We can represent Enhanced Suffix Array of string $S$ as two tables:
\begin{enumerate}
  \item \emph{suftab} --- basic suffix array, i.e.\ array of integers in the range $0\dots n$, where $n = |S|$, specifying the lexicographic ordering of the $n + 1$ suffixes of string $S\$ $.
  \item \emph{lcptab} --- array of integers in the range $0\dots n$, where lcp-value $lcptab[i]$ is the length of the longest common prefix of $S_{subtab[i]}$ and $S_{subtab[i-1]}$, for $1 \leq i \leq n$ and $lcptab[0] = 0$.
\end{enumerate}

We also need to introduce the concept of the \emph{lcp-intervals} and \emph{lcp-interval trees}. The notion of lcp-intervals was first used in~\cite{enhancedsuffixarrays}.
\begin{definition}
An interval $[i..j]$, $0 \leq i < j \leq n$, is an lcp-interval of lcp-value $\ell$ if

\begin{enumerate}
\item $lcptab[i] < \ell$,
\item $lcptab[k] \geq \ell$ for all $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[k] = \ell$ for at least one $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[j + 1] < \ell$.
\end{enumerate}
\end{definition}

Next we define parent-child relationships between lcp-intervals and thus an lcp-interval tree~\cite{enhancedsuffixarrays}.

\begin{definition}
  An $m$-interval $[l..r]$ is said to be \emph{embedded} in an $\ell$-interval $[i..j]$ if it is a~subinterval of $[i..j]$ (i.e., $i \leq l < r \leq j $) and $m > \ell$.\footnote{Note that we cannot have both $i = l$ and $r = j$ because $m > \ell$.}
  The $\ell$-interval $[i..j ]$ is then called the interval \emph{enclosing} $[l..r]$. If $[i..j]$ encloses $[l..r]$ and there is no interval embedded in $[i..j]$ that also encloses $[l..r]$, then $[l..r]$ is called a child interval of $[i..j]$.
\end{definition}

The lcp-interval tree is a conceptual tree, i.e.\ we do not need to construct it explicitly in any of the algorithms. The root of the tree is the $0$-interval $[0..n]$ and the rest of tree is defined by the parent-child relationship from the definition.

\todo{example, figures}

The lcp-interval has an important property to the $k$-mer counting problem. An $\ell$-interval $[i..j]$ represents a string of length $\ell$ occurring $j - i + 1$ times in S~\cite{tallymer}. To count the $k$-mer occurrence counts, we need to read the occurrence counts from the lcp-interval tree.

We can compute the $k$-mer histogram $H_k$ using the algorithm for enumeration lcp-interval tree nodes from~\cite{enhancedsuffixarrays}. The algorithm traverses the tree in bottom up order, i.e\ a node is enumerated only after all nodes in its subtree have been enumerated.

We start with processing the singleton intervals $[i..i]$. The singleton interval $[i..i]$ corresponds to the suffix $S_{suftab[i]}$. Let $d$ be the lcp-value of the parent node of the interval $[i..i]$, $S[suftab[i]..suftab[i]+k-1]$ is a $k$-mer occurring exactly once in $S$ if and only if $d < k$ and $suftab[i]+k < n$. We increment $H_k[1]$ by one for each interval satisfying the former conditions.

Then we process the other $\ell$-intervals $[i..j]$, (except the root node). Again, let $d$ be the lcp-value of the parent of interval $[i..j]$, $S[suftab[j]..suftab[j] + k - 1]$ is a $k$-mer occurring exactly $j - i + 1$ times in $S$ if and only if $d < k \leq \ell$. We increment $H_k[j-i+1]$ by one for each interval satisfying the former conditions.

One of the advantages of this approach, is that we can compute $H_k$ for different $k$ at once --- at each step we increment specific positions in all corresponding histograms $H_k$ for each $k$ we are interested. This approach is a core of method used in~\cite{tallymer}, but the suffix arrays are used also in other tools e.g.~\cite{jellyfish}.

% To address the problem of counting $k$-mers in multiple sequences (reads), we can concatenate all reads to one sequence, using dividers $\$_1\dots \$_r$. To compute occurrences for large sequences, we need to split the sequence into multiple shorter non-overlaping sequences and use divide and conquer algorithm described in~\cite{tallymer}.

\section{Conclusion}

We briefly summarized the most important approaches to the $k$-mer counting problem. We also described the key data structures and optimizations for the $k$-mer counting --- namely enhanced suffix arrays, lock free hash tables, bloom filters and count-min sketch.

There are a lot of available tools for $k$-mer counting varying in their approach to the problem and in their functionality. Tallymer uses suffix arrays, Jellyfish uses parallel approach, BFCounter uses bloom filter to filter out the unique $k$-mers from the hash table, Khmer uses count-min sketch as it's data structure and DSK uses the partitioning approach.

The tools have various limitations --- for example DSK does not provide random access to the counts and Khmer does not count $k$-mers exactly.
The comparison of the tools can be found in~\cite{khmer}.
% cosi o pouziti a pod
