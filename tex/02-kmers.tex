% chktex-file 1
\chapter[K-mer Counting]{K-mer Counting and K-mer abundance histograms}

% intor o kmeroch a podobne

\Kmer is a substring of length exactly~$k$. In \emph{\kmer counting} we take sequencing reads as an input and we want to know for each \kmer occurring in the reads, how many occurrences does it have. If we know all the counts of the \kmers, we can compute a \kmer histogram.
The \emph{$k$-mer abundance histogram $H_k$ (shortly \kmer histogram)} is an array, where the $i$-th element (denoted by $H_k[i]$) is the number of unique $k$-mers occurring in the data exactly $i$ times. The size of the histogram denoted by $|H_k|$ is maximal $i$ such that $H_k[i] > 0$.

Counting of the $k$-mer (substring of length~$k$) occurrences is one of the key problem in many applications in bioinformatics.
For example, it was used to separate several individual
genomes when sequencing a metagenomic mixture of different microbes isolated from a certain environment~\cite{Wu2011, Wang2012}.
It is also used in error correction algorithms e.g.\ for correcting errors prior the DNA assembly~\cite{Pevzner2001}, sequence aligners~\cite{edgar2004muscle} and repeat detectors~\cite{caponnetto2013efficiency}.
It is also suitable for various sequence read analysis and genome size estimation\cite{covest, williams, waterman}. In many cases, computation of the $k$-mer abundance histogram is sufficient.
We will use the \kmer histograms in~chapter~\ref{chap:genomesize} for estimating the coverage and the size of a genome.

NGS data, usually consists of many reads. Although these reads typically cover the target genome multiple times, sequencing errors in the reads result in many unique $k$-mers.
This causes a high memory usage of naive approach to this problem.
\todo{nejaky priklad s cislami}

There are several existing tools addressing this issue for $k$-mer counting or $k$-mer histogram computation, for example Tallymer\cite{tallymer}, JELLYFISH\cite{jellyfish}, BFCounter\cite{bfcounter}, DSK\cite{dsk} and Khmer\cite{khmer}. Each tool must handle the trade-off between speed, memory usage, disk usage, precision and random access capability. Most approaches to the $k$-mer counting problem, use hash tables, Bloom filters or suffix arrays.

\section{Simple Hashing Approach}

Let us first briefly explain the naive solution, so we can extend and compare it to the methods used in selected published tools.

Suppose we want to count occurrences of $k$-mers in sequence $S$.
We create a hash table $T$, in which \kmers will be keys and their counts stored as values, and then scan the sequence $S$.
At each position $i$, we take the $k$-mer $K_i$ corresponding to that  position, compute $hash(K_i)$ and increase the corresponding item in the table, after handling possible collisions.

We can then query the hash table for the number of occurrences of a~particular $k$-mer, or iterate through the whole table to compute the histogram.

For small genomes with very low error rate, this approach is sufficient. However, higher error rate produces more unique \kmers and together with large genomes makes the hash table so big it doesn't fit in computer's memory. In addition, the general hash table is not parallelizable, which makes this approach slow.

\section[Filtering Unique K-mers]{Filtering Unique K-mers using Bloom Filter}

In NGS data, there are a lot of errors in the reads. Error rate in NGS data is usually about 1--5\%. This means that probability of 20-mer containing error with error rate 1\% is $1 - 0.99^{20} \approxeq 0.18$ i.e.\ about 18\% of $k$-mers from same position in DNA will be different. Together with low probability of $k$-mers from different location be the same and low probability that errors cause $k$-mers from different positions to be same, we get most $k$-mers unique in low to mid coverage data. If we do not store these $k$-mers in hash table, we can save a lot of memory. We can use more memory efficient, probabilistic data structure called \firstUseOf{Bloom Filter}\cite{bloomfilter} to store unique $k$-mers.

The Bloom filter supports two operations --- \method{add} for adding an element to the Bloom filter and \method{test} for testing whether an element is in Bloom filter. The following property holds for the \method{test} operation:
\begin{itemize}
  \item If the element is set in the Bloom filter, returns \emph{true}.
  \item If the element is not set it returns \emph{false} with high probability.
\end{itemize}
If the test method returns true, the element is probably set in the filter, otherwise it is definitely not there.

The Bloom Filter basically consists of a bit vector of length $n$ and $m$ hashing functions, each transforming the element to the position in the bit vector. The parameters directly affect the false positive rate of the Bloom filter.
The \method{add} operation computes $m$ positions from $m$ hashing functions applied to the element and sets them to one.
The \method{test} operation computes $m$ positions from the element and checks whether each of them is one. If any of them is zero, returns false, otherwise returns true. Further details are described in~\cite{bloomfilter}.

We can use Bloom Filter to store information about all $k$-mers we have seen at some point of computation. We just add every $k$-mer to the Bloom filter during the computation. Before adding the $k$-mer, we first check if the $k$-mer is already in the Bloom filter. If yes, it means we (probably) have seen this $k$-mer before and we also add it to the hash table. This way we get approximate counts for each $k$-mer.
With one more pass we can get exact counts. We go through the $k$-mers once again and check if they are not in hash table. If not, they are certainly unique. More details are in~\cite{bfcounter}.

\section[Probabilistic K-mer Counting]{Probabilistic K-mer Counting Using Count-Min Sketch}

The Count-Min sketch\cite{countminsketch} data structure enables counting of objects in sublinear space. It supports two operations:
\begin{itemize}
  \item \method{incrementcount(x)} %chktex 36
  \item \method{getcount(x)} %chktex 36
\end{itemize}

The data structure consists of $m$ hash tables, with different hash functions.
The \method{incrementcount(x)} operation computes hash of $x$ using each hash function and increments corresponding counters in all hash tables. The \method{getcount} operation computes hash of $x$ using each hash function and takes the minimum of all corresponding values in hash tables as the result. %chktex 36

If there is a collision in each table for a particular object, the \method{getcount} operation may return greater value for such object. This false positive rate depends on the collision probability, which is proportional to the number of hash tables and their sizes.

This data structure can be directly used to the $k$-mer counting problem. As it does not store the $k$-mers itself, it allows in memory $k$-mer counting and querying, which is useful for streaming applications.
More details about false positive rate estimation and comparison to other approaches can be found in~\cite{khmer}.

\section[Other Improvements]{Other Improvements of the Hashing Approach}

There are two more improvements of $k$-mer counting methods we have found in the papers about $k$-mer counting.

\paragraph{} The first is memory usage improvement using explicit disk-memory trade-off. The \emph{Disk Streaming of K-mers (DSK)} algorithm\cite{dsk} exploits the fact that the multi-set of $k$-mers can be partitioned. Each partition can be loaded in the memory separately.

The algorithm uses a simplifying assumption: Let $d$ be the total number of distinct $k$-mers in the input; we assume that the number of distinct $k$-mers having a given hash value is at most $\lceil d/H \rceil$, where $H$ is the size of the hash table\cite{dsk}.

The algorithm works in $n_{iters} = \lceil vb/D\rceil$ iterations, where $v$ is total number of $k$-mers, $b$ is number of bits used to store the $k$-mer ($b = 2^{\lceil\log_2(2k)\rceil}$). Each $k$-mer is written to the disk only in $(h(m) \mod n_{iters})$-th iteration.

At each iteration algorithm maintains $n_p = \lceil\frac{vb+32}{0.7n_{iters}M}\rceil$ lists of $k$-mers.
It goes through all the $k$-mers in the input and writes the $k$-mers valid for the iteration to the $h(m)/n_{iters} \mod n_p$-th list on the disk.
Then for each list on the disk it constructs in-memory hash table storing the counts for each $k$-mer in the disk.
Finally it outputs every key-value pair in the hash table.

This algorithm is proved to use $D$ bits of disk space and $M$ bits of memory\cite{dsk}.

\paragraph{} The second is speed improvement by parallelism. To enable parallelism, we need to allow concurrent access to the key data structure for $k$-mer counting --- hash table, which must be shared between threads. The threads have to be able to read from and write to hash table at once, without locking the access to it. Otherwise we would loose the advantage of parallel execution, because other threads would need to wait the lock to be released. The thread safe, lock-free hash table implementation was presented in this~\cite{jellyfish}. It uses \emph{compare-and-swap} (CAS) instruction which is widely available in the modern processors. The CAS instruction does these operations atomically:
\begin{lstlisting}
def cas(location, oldvalue, newvalue):
  currentvalue = read(location)
  if currentvalue == oldvalue:
    set(location, newvalue)
  return currentvalue
\end{lstlisting}
If two threads access same memory location simultaneously, one of them will fail to set new value. In this case, the thread must take appropriate action.
It can retry the CAS operation until it is successful.

The algorithm for $k$-mer counting hash table works in two steps:
First it computes the location of the $k$-mer in the hash table and then it increments the value. Both of the operations can be done thread-safe without locks using the CAS instruction.

\section[Enhanced Suffix Arrays]{Computing K-mer Histogram using Enhanced Suffix Arrays}

\emph{Suffix tree}\cite{suffixtree} is a data structure widely used in string processing. Basically, it is a compressed lexicographical tree build from all suffixes of a string. Let's have an alphabet $\Sigma$ and a string $S \in \Sigma^*$. Usually, for easier construction and manipulation, suffix tree is constructed from string $S\$$ instead of $S$, where $\$$ is a lexicographically smaller than every character in $\Sigma$. The suffix tree can be constructed in $O(|S|)$ time and space complexity~\cite{suffixtree}.

\emph{Suffix arrays}\cite{suffixarray} were developed as more space efficient replacement of suffix trees. The suffix array requires only $4n$ bytes\footnote{We suppose, that we can use 32bit integer values to store the data.} in its basic form, whereas a suffix tree require $20n$ bytes in worst case~\cite{kurtz1999reducing}. The suffix array is an array containing indices of lexicographically sorted suffixes of $S\$$ (similarly to suffix arrays we append $\$$ end marker to the original string $S$). Similarly to the suffix tree, a suffix array can be constructed in linear time~\cite{karkkainen2003simple, kim2003linear, ko2003space}. The suffix arrays are also faster due to poor locality of memory reference of the Suffix Trees, causing efficiency loss on cached processor architectures.

With basic suffix arrays it was not possible to directly replace the suffix tree with the suffix array in any suffix tree based string processing problem and maintain its time complexity. For example, using only the basic suffix array, it takes $O(m \log n)$ time in the worst case to answer decision queries of the type ``Is P a substring of S?'', where $m = |P|$.

\firstUseOf{Enhanced Suffix Arrays} are Suffix Arrays enhanced with some additional information --- in this case with the \firstUseOf{lcp (longest common prefix)} information.
In contrast to the Suffix Trees, enhanced suffix arrays use less memory (the lcp table can be stored in additional $4n$ bytes) and they can replace suffix trees in every algorithm maintaining the same time complexity~\cite{enhancedsuffixarrays}.

We can represent Enhanced Suffix Array of string $S$ as two tables:
\begin{enumerate}
  \item \emph{suftab} --- basic suffix array, i.e.\ array of integers in the range $0\dots n$, where $n = |S|$, specifying the lexicographic ordering of the $n + 1$ suffixes of string $S\$ $.
  \item \emph{lcptab} --- array of integers in the range $0\dots n$, where lcp-value $lcptab[i]$ is the length of the longest common prefix of $S_{subtab[i]}$ and $S_{subtab[i-1]}$, for $1 \leq i \leq n$ and $lcptab[0] = 0$.
\end{enumerate}

We also need to introduce the concept of the \firstUseOf{lcp-intervals} and \firstUseOf{lcp-interval trees}. The notion of lcp-intervals was first used in~\cite{enhancedsuffixarrays}.
\begin{definition}
An interval $[i..j]$, $0 \leq i < j \leq n$, is an lcp-interval of lcp-value $\ell$ if

\begin{enumerate}
\item $lcptab[i] < \ell$,
\item $lcptab[k] \geq \ell$ for all $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[k] = \ell$ for at least one $k$ with $i + 1 \leq k \leq j$,
\item $lcptab[j + 1] < \ell$.
\end{enumerate}
\end{definition}

Next we define parent-child relationships between lcp-intervals and thus an lcp-interval tree\cite{enhancedsuffixarrays}.

\begin{definition}
  An $m$-interval $[l..r]$ is said to be \emph{embedded} in an $\ell$-interval $[i..j]$ if it is a~subinterval of $[i..j]$ (i.e., $i \leq l < r \leq j $) and $m > \ell$.\footnote{Note that we cannot have both $i = l$ and $r = j$ because $m > \ell$.}
  The $\ell$-interval $[i..j ]$ is then called the interval \emph{enclosing} $[l..r]$. If $[i..j]$ encloses $[l..r]$ and there is no interval embedded in $[i..j]$ that also encloses $[l..r]$, then $[l..r]$ is called a child interval of $[i..j]$.
\end{definition}

The lcp-interval tree is a conceptual tree, i.e.\ we do not need to construct it explicitly in any of the algorithms. The root of the tree is the $0$-interval $[0..n]$ and the rest of tree is defined by the parent-child relationship from the definition.

\todo{example, figures}

The lcp-interval has an important property to the $k$-mer counting problem. An $\ell$-interval $[i..j]$ represents a string of length $\ell$ occurring $j - i + 1$ times in S~\cite{tallymer}. To count the $k$-mer occurrence counts, we need to read the occurrence counts from the lcp-interval tree.

We can compute the $k$-mer histogram $H_k$ using the algorithm for enumeration lcp-interval tree nodes from~\cite{enhancedsuffixarrays}. The algorithm traverses the tree in bottom up order, i.e\ a node is enumerated only after all nodes in its subtree have been enumerated.

We start with processing the singleton intervals $[i..i]$. The singleton interval $[i..i]$ corresponds to the suffix $S_{suftab[i]}$. Let $d$ be the lcp-value of the parent node of the interval $[i..i]$, $S[suftab[i]..suftab[i]+k-1]$ is a $k$-mer occurring exactly once in $S$ if and only if $d < k$ and $suftab[i]+k < n$. We increment $H_k[1]$ by one for each interval satisfying the former conditions.

Then we process the other $\ell$-intervals $[i..j]$, (except the root node). Again, let $d$ be the lcp-value of the parent of interval $[i..j]$, $S[suftab[j]..suftab[j] + k - 1]$ is a $k$-mer occurring exactly $j - i + 1$ times in $S$ if and only if $d < k \leq \ell$. We increment $H_k[j-i+1]$ by one for each interval satisfying the former conditions.

One of the advantages of this approach, is that we can compute $H_k$ for different $k$ at once --- at each step we increment specific positions in all corresponding histograms $H_k$ for each $k$ we are interested. This approach is a core of method used in~\cite{tallymer}, but the suffix arrays are used also in other tools e.g.~\cite{jellyfish}.

% To address the problem of counting $k$-mers in multiple sequences (reads), we can concatenate all reads to one sequence, using dividers $\$_1\dots \$_r$. To compute occurrences for large sequences, we need to split the sequence into multiple shorter non-overlaping sequences and use divide and conquer algorithm described in\cite{tallymer}.

\section{Conclusion}

We briefly summarized the most important approaches to the \kmer counting problem. We also described the key data structures and optimizations for the \kmer counting --- namely enhanced suffix arrays, lock free hash tables, bloom filters and count-min sketch.

There are a lot of available tools for \kmer counting varying in their approach to the problem and in their functionality. Tallymer uses suffix arrays, Jellyfish uses parallel approach, BFCounter uses bloom filter to filter out the unique \kmers from the hash table, Khmer uses count-min sketch as it's data structure and DSK uses the partitioning approach.

The tools have various limitations --- for example DSK does not provide random access to the counts and Khmer does not count \kmers exactly.
The comparison of the tools can be found in~\cite{khmer}.
% cosi o pouziti a pod
