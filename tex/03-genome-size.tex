% chktex-file 1
\chapter[Genome Size and Coverage]{Genome Size and Coverage Estimation}
\label{chap:genomesize}

The genome size is one of the fundamental questions related to the sequenced genome.
The estimate of genome size can be very useful in experiment design in biology. If it is possible to estimate the genome size from low coverage sequencing data, biologists can do this estimate with low cost prior the experiment\todo{aky experiment} and use the information to better design the experiment itself.

The \emph{coverage} of genome by the sequencing data is the average number of sequencing reads covering a single position of the genome. The coverage $c$ is closely related to the genome size and can be computed as a ratio of sequencing data size (the sum of lengths of all reads in sequencing data $L\cdot N$, where $L$ is the read length and $N$ is the number of reads) and the genome size $G$.
$$c = \frac{NL}{G}$$
The estimate of a coverage can be used in various sequencing data analyses, e.g.\ repeat analysis and analyses of gene families (see chapter~\ref{chap:repeatsfamilies}). \todo{ked bude 4 kapitola, tak toto prepisat podla topho ako to tam pouzijeme}

Ideally the result of sequencing would be a complete sequence of each chromosome and we could compute the size of the genome directly.
However, in most cases we cannot assemble the genome only from the short reads provided by the new generation sequencing machines.
Even if we are able to build a draft (partial) assembly, there are usually some missing parts and assembly errors around repetitive sequences. This errors can have significant influence on the genome size estimates. In addition, to do a genome assembly, we need to have high coverage data, which is expensive for very large genomes. For example, the panda genome was the first mammalian genome sequenced
by the next-generation sequencing technologies~\cite{li2010panda}. The authors have assembled around $2.3$Gb of sequence to contigs and scaffolds, but using
several methods, they estimate the genome size to be around $2.4$Gb.

Current research in genome size estimation focuses on direct estimation from sequencing data. This approach has several advantages: it is possible to compute the estimate faster because it avoids the assembly process and it also avoids any biases introduced by genome assemblers.

\section[K-mer Histograms]{Using K-mer Histograms for Genome Size Computation}\label{sec:kmerhist}

% This can be done statistically by looking at sequence overlaps.
Ideally we would like to compute for each base in the original genome how many times it is covered by reads. Since genome assembly is not available, we will approximate this process using reads only. We will consider the context of each base, so that we can tell which bases in different reads corresponds to the same position in the original genome. We can do this by using $k$-mers (with $k$ such that the probability of two identical $k$-mers corresponding to different positions in the genome is very low).

\subsection{Simplified Scenario}

Now let us introduce several approaches for computing a genome size and a coverage from \kmer histograms.
Let $c$ be the average number of reads covering a single base of the genome. When we look at a $k$-mer starting at particular position, not all reads covering its start will cover the entire $k$-mer.
Therefore we have to adjust the coverage for $k$-mers.
If all reads have the same length $r$, the average coverage of a single $k$-mer will be $c_k = c (r - k + 1)/r$.
In the following text, we will work with collection of \kmers instead of reads and use $c_k$ coverage.

Firstly let us assume there are no errors in the reads and every \kmer in the original sequence is unique.
We also assume that in NGS sequencing the starting positions of reads are selected randomly from the genome.
Under these assumptions, the probability that a particular position or \kmer is covered by random read is equal for all positions or \kmers. Probability that a particular position or \kmer is covered by $j$ reads is then given by the binomial distribution. The binomial distribution can be aproximated by the \emph{Poisson distribution} for large values, which enables effective computation of the probability.
The probability, $p_j$, that particular \kmer is covered by $j$ reads can be computed from Poisson distribution as follows:
$$ p_j =  \frac{c_k e^{-c_k}}{j!}.$$
The $c_k$ is the average coverage and thus we can set the mean in the Poisson distribution to $c_k$.
However, the abundance of some \kmers under this distribution may be zero and we will not observe them in the spectrum. Hence, the probability that a given \kmer from the genome occurs in the sequencing data exactly $j$ times is $$p_j = \frac{c_k^j e^{-c_k}}{j! (1-e^{-c_k})}.$$
This distribution is called \emph{truncated Poisson distribution} and it renormalizes the probabilities of the Poisson distribution with mean $c_k$ by the probability of obtaining a value greater then zero in Poisson distribution, which is $1-e^{-c_k}$.

Given the abundance histogram, we can compute the likelihood of $c_k$, with respect to the histogram.
$$L(H_k | c_k) = \sum_{j=1}^m h_k[j] \log p_j.$$
We can then estimate the best $c_k$ by maximal likelihood estimation.


\subsection{Modeling Repeats}\label{subsec:repeatmodles}

Real genomes contains a lot of repeats (\todo{priklad, niekde bola tusim kvasinka 30\%}) and the sequencing machines produce a lot of errors (\todo{priklad, mozno ten isty co bude v predoslej kapitole}). To provide a good estimate for real data, the estimators have to consider this issues.

\todo{ako modeluju repeaty}

The first research in this field focused on handling repeats and contained very simple error handling approaches. In~\cite{waterman}, they estimated the genome size and coverage based on probabilistic models of repeat content of the genome and used EM algorithm to fit the parameters of the model to the data.

In~\cite{williams} authors analyzed \kmer histograms multiplied by an abundance of columns (i.e.\ let $H_k$ be the \kmer histogram and $n = |H_k|$, the transformed histogram $W_k = 1 \cdot H_k[1], 2\cdot H_k[2], \dots, n \cdot H_k[n]$). They looked for the principal peek in $W_k$ which represented the coverage of single copy parts of genome --- \emph{the modal coverage} --- and other peeks at integer multiplies of modal coverage which represented the sequence repeats.
They filtered errors simply by truncating the low abundance part of the histogram.

According to~\cite{waterman} and~\cite{williams}, to be able to distinguish and model repeats, the higher coverage is necessary. As this method focuses on low coverage estimation, it models repeats in very simple way. It uses 3 parameters --- for single copy parts of genome, for parts which are repeated exactly twice and all others. The proportions of the \kmers repeated more than twice are modeled using a geometric distribution.

The model distribution is then a mixture of probabilities from the previous model, where $c_k$ is replaced by $o \cdot c_k$ and $o$ is number of the \kmer.
This method allows genome size and coverage estimation for low coverage data ($0.5\times$) or higher error rates (10\%).

\subsection{Modeling Errors}

In~\cite{waterman} and \cite{williams} they handled the sequencing errors very simply. In~\cite{waterman} they did not consider the errors in the data at all and run their program on error corrected data. This might add some biases from the error correcting program, cause loss of part of data and it also increases the time complexity of the process. In~\cite{williams} they filtered errors simply by truncating the low abundance part of the histogram. This requires higher coverage.

In our paper\cite{covest}, we focused on modeling errors directly. We provided several probabilistic models graded by complexity.
The simplest model considered errors contributing only to abundance of one in the histogram (i.e.\ every error creates a unique $k$-mer) and does not consider any repeats. The model is a mixture of two distributions --- the distribution for erroneous \kmers and the distribution for error free \kmers. The abundances of the error free \kmers are generated from the truncated Poisson distribution with mean $c'$. Let $\epsilon$ be the error rate, we can compute $c_k$ from $c'$ by dividing it by the probability that \kmer contains no error: $c_k = c'/{(1-\epsilon)}^k$.

The probability that the same error happen multiple times increases with increasing coverage. Hence the second model considers errors contributing also to higher abundances.
The second model is a mixture of truncated Poisson distributions with means $\lambda_s \forall s \in \{0\dots k\}$, where $s$ is a number of errors in each \kmer considered in the partial distribution. This model still does not consider any repeats in the original sequence.
This is fixed in final model, where repeats were modeled by geometric distribution (see section \ref{subsec:repeatmodles}).

\todo{short conclusion}

\subsection{Testing the Models}

\todo{ako sa testuju}
\todo{sumar z vysledkov clankov}

\section{Conclusion}

In this chapter we dealt with a problem of estimating the genome size and the coverage. We showed two main approaches to this problems based on \kmer histogram analysis.

The first approach was based on precise modeling of repeat content and omitted the error modeling, whereas the second approach was based on precise error modeling and simple repeat modeling.
The first approach works well on high coverage data and is more suitable for genomes, where the repeat content is very different from the simple model in the second approach.

The second approach works well even in very low coverage data ($0.5\times$) or with highly erroneous data ($10\%$ error rate). This approach is very universal and easily extensible to more difficult models.

The possibility of estimating the genome size and the coverage from low coverage data is very useful for cheaper experiment design and for analysis of large genomes, such as plant genomes where the sizes are in tens of gigabases.
