% chktex-file 1
\chapter[Genome Size and Coverage]{Genome Size and Coverage Estimation}
\label{chap:genomesize}

The genome size is one of the fundamental questions related to the sequenced genome.
The estimate of genome size can be very useful in experiment design in biology. If it is possible to estimate the genome size from low coverage sequencing data, biologists can do this estimate with low cost prior the experiment\todo{aky experiment} and use the information to better design the experiment itself.

The \emph{coverage} of genome by the sequencing data is the average number of sequencing reads covering a single position of the genome. The coverage $c$ is closely related to the genome size and can be computed as a ratio of sequencing data size (the sum of lengths of all reads in sequencing data $L\cdot N$, where $L$ is the read length and $N$ is the number of reads) and the genome size $G$.
$$c = \frac{NL}{G}$$
The estimate of a coverage can be used in various sequencing data analyses, e.g.\ repeat analysis and analyses of gene families (see chapter~\ref{chap:repeatsfamilies}). \todo{ked bude 4 kapitola, tak toto prepisat podla topho ako to tam pouzijeme}

Ideally the result of sequencing would be a complete sequence of each chromosome and we could compute the size of the genome directly.
However, in most cases we cannot assemble the genome only from the short reads provided by the new generation sequencing machines.
Even if we are able to build a draft (partial) assembly, there are usually some missing parts and assembly errors around repetitive sequences. This errors can have significant influence on the genome size estimates. In addition, to do a genome assembly, we need to have high coverage data, which is expensive for very large genomes. For example, the panda genome was the first mammalian genome sequenced
by the next-generation sequencing technologies~\cite{li2010panda}. The authors have assembled around $2.3$Gb of sequence to contigs and scaffolds, but using
several methods, they estimate the genome size to be around $2.4$Gb.

Current research in genome size estimation focuses on direct estimation from sequencing data. This approach has several advantages: it is possible to compute the estimate faster because it avoids the assembly process and it also avoids any biases introduced by genome assemblers.

\section[K-mer Histograms]{Using K-mer Histograms for Genome Size Computation}\label{sec:kmerhist}

% This can be done statistically by looking at sequence overlaps.
Ideally we would like to compute for each base in the original genome how many times it is covered by reads. Since genome assembly is not available, we will approximate this process using reads only. We will consider the context of each base, so that we can tell which bases in different reads corresponds to the same position in the original genome. We can do this by using $k$-mers (with $k$ such that the probability of two identical $k$-mers corresponding to different positions in the genome is very low).

\subsection{Simplified Scenario}

Now let us introduce several approaches for computing a genome size and a coverage from \kmer histograms.
Let $c$ be the average number of reads covering a single base of the genome. When we look at a $k$-mer starting at particular position, not all reads covering its start will cover the entire $k$-mer.
Therefore we have to adjust the coverage for $k$-mers.
If all reads have the same length $r$, the average coverage of a single $k$-mer will be $c_k = c (r - k + 1)/r$.
In the following text, we will work with collection of \kmers instead of reads and use $c_k$ coverage.

Firstly let us assume there are no errors in the reads and every \kmer in the original sequence is unique.
We also assume that in NGS sequencing the starting positions of reads are selected randomly from the genome.
Under these assumptions, the probability that a particular position or \kmer is covered by random read is equal for all positions or \kmers. Probability that a particular position or \kmer is covered by $j$ reads is then given by the binomial distribution. The binomial distribution can be aproximated by the \emph{Poisson distribution} for large values, which enables effective computation of the probability.
The probability, $p_j$, that particular \kmer is covered by $j$ reads can be computed from Poisson distribution as follows:
$$ p_j =  \frac{c_k e^{-c_k}}{j!}.$$
The $c_k$ is the average coverage and thus we can set the mean in the Poisson distribution to $c_k$.
However, the abundance of some \kmers under this distribution may be zero and we will not observe them in the spectrum. Hence, the probability that a given \kmer from the genome occurs in the sequencing data exactly $j$ times is $$p_j = \frac{c_k^j e^{-c_k}}{j! (1-e^{-c_k})}.$$
This distribution is called \emph{truncated Poisson distribution} and it renormalizes the probabilities of the Poisson distribution with mean $c_k$ by the probability of obtaining a value greater then zero in Poisson distribution, which is $1-e^{-c_k}$.

Given the abundance histogram, we can compute the likelihood of $c_k$, with respect to the histogram.
$$L(c_k | H_k) = \sum_{j=1}^m H_k[j] \log p_j.$$
We can then estimate the best $c_k$ by maximal likelihood estimation.


\subsection{Modeling Repeats}\label{subsec:repeatmodles}

\todo{toto este treba ucesat poriadne}

Real genomes contains a lot of repeats (\todo{priklad, niekde bola tusim kvasinka 30\%}) and the sequencing machines produce a lot of errors (\todo{priklad, mozno ten isty co bude v predoslej kapitole}). To provide a good estimate for real data, the estimators have to consider this issues. Let us deal with handling repeats first.

% Methods described in~\cite{waterman} and ~\cite{williams} were focused on handling repeats.
\paragraph{}The first method~\cite{waterman} is based on simplified scenario described above. Suppose, the \kmer $w$ occurs $n(w)$ times in the sequence. Let $x_i(w)$ be the number of reads covering the $i$-th occurrence of $w$. Similarly to the simplified scenario, $x_i(w)$ can be modeled by the Poisson distribution with mean $c_k$, which is the base coverage. The the total number of occurrences of $w$ in reads can be than modeled as $$\sum_{i=1}^{n(w)} x_i(w) = \sum_{i=1}^{n(w)} Pois(c_k) = n(w) Pois(c_k).$$

Suppose there are $n_f$ repeat families in the data. The number of occurences of any \kmer in $i$-th family, $F_i$, is a Poisson random variable with parameter $a_i c_k$. Moreover, we assume that proportion of different \kmers in $F_i$ of all unique \kmers in the original sequence is $\alpha_i$.
We then need to solve following problem: we have set of samples from a mixed Poisson distribution and some of the samples are dependent. We want to estimate $a_i,\,\alpha_i$ for $i = 1\dots n_f$.

We can use following formulas to estimate the parameters~\cite{waterman}:
\begin{align*}
a_i c_k & = \frac{\displaystyle\sum_w n(w) \Pr[w \in F_i | n(w)]}{\displaystyle\sum_w \Pr[w \in F_i | n(w)]},\\[1.5ex]
\alpha_i & = \frac{\displaystyle\sum_w \Pr[w \in F_i | n(w)]}{\displaystyle\sum_{j=1}^{n_f}\sum_w \Pr[w \in F_j | n(w)]},\\[1.5ex]
\Pr[w \in F_i | n(w)] & = \frac{\alpha_i}{\displaystyle\sum_{j=1}^{n_f} \alpha_j \left(\frac{a_j}{a_i}\right)^{n(w)} e^{(\alpha_i-a_j)c_k}}.
\end{align*}

Note that this method uses standard Poisson distribution for their estimates. Thus we need to calculate the number of \kmers which are not present in our data. We can use following formula to estimate it~\cite{waterman}:
$$H_k[0] = \frac{\displaystyle\sum_{i=1}^{n_f}\alpha_i e^{-a_i c_k} \sum_{i=1}^{|H_k|}H_k[i]}{\displaystyle\sum_{i=1}^{n_f}\alpha_i\left(1 - e^{-a_i c_k}\right)}$$

We can use EM algorithm to obtain the values of the parameters $a_i,\, \alpha_i$ and $c_k$ from above equations, which iteratively computes the parameters based on values of the parameters from previous step and stops when they converge or change very little.

If we know the parameters we can compute the genome size using one of the following formulas~\cite{waterman}:

\begin{flalign*}
&& G & = \sum_{j=1}^{n_f} a_j\alpha_j \sum_{j=1}^{|H_k|}H_k[j];&&\\
\text{or} &&\\
&& G & = \frac{N(L-k+1)}{\min\{a_j c_k: j = 1,\dots,|H_k|\}}&&.
\end{flalign*}

\paragraph{}The second method~\cite{williams} exploits the fact that if the \emph{modal} (single-copy) coverage is $c$, than coverage of repetitive region would be an integer multiply of $c$.

Recall that if \kmers were randomly sampled from a genome without repeats, the shape of the \kmer histogram would be a single Poisson distribution.
$$H_k[j] \approx a_1 Poisson(j; c_k)$$
where $H_k[j]$ is the number of \kmers observed $o$ times, $a_1$ is the number of unique \kmers, and $c_k$ is the \kmer coverage.

Let $a_o$ be the number of \kmers in the original genome, which repeat in the genome $o$ times. As we already have mentioned, distribution of abundances of the \kmers with abundance $o$ would be $Pois(o*c_k)$ and the number of \kmers occurring in the data $j$ times would be $a_o Poisson(j; o*c_k)$. Hence, if we put all sequences together, we sum up the distribution for each level of abundance and the shape of the histogram would be~\cite{williams}:
$$H_k[j] \approx \sum_o a_o Poisson(j; o\cdot c_k).$$
This can be generalized to over-dispersed Poisson shapes by introducing a single over-dispersion shape parameter $s$ to allow distributions with excess variance~\cite{williams}:
$$P(j; c_k, \{a_o\}, s) = \sum_o a_o NegBinomial(j; \mu=c_k\cdot o;\sigma^2=s/o).$$
%treba spomenut dovod?? ze im poisson mixture tak dobre nefitoval akp toto?
This model can be used to infer the parameters by maximizing the likelihood for the observed histogram $H_k$:
$$L(c_k, \{a_o\}, s | H_k) = \sum_j Poisson(H_k[j]; P(j; c_k, \{a_o\}, s)).$$

The number of unique \kmers is the sum of the $\{a_o\}$ coefficients and the size of the genome can be estimated as the sum of the products of the unique \kmers and their relative abundances:
$$G = \sum_o o\cdot a_o.$$

\paragraph{} According to~\cite{waterman} and~\cite{williams}, to be able to distinguish and model repeats, the higher coverage is necessary. In our paper\cite{covest}, we focused on estimates from low coverage data, thus the approaches described above would not be applicable. Instead of that we decided to model the repeats very simply.

Let $p_j(c)$ be the probability that a particular \kmer occurs $j$ times in the data given the coverage $c$. The probability that a \kmer from sequence repeated $o$ times occures $j$ times in the data is then $p_{o,j} = p_j(o\cdot c)$.
Similarly to the previous method, we can model the overall probability distribution as a mixture distribution:
$$p_j = \sum_{o=1}^\infty \beta_o p_{o,j}.$$

We assumed that the probability that sequence repeats $n$ times decreases with increasing $n$. We decided to model repeats by the geometric distribution. We also decided to model the single-copy sequences and sequences which repeats exactly twice in the genome separately. We introduced three parameters --- $q_1$, $q_2$, and $q$ --- for the single-copy, two-copy and the parameter for geometric distribution for higher abundance sequences.

We can then use these parameters to define a mixture of distributions to express the $\beta_o$ parameters as follows:
$\beta_1 = q_1$, $\beta_2 = (1-q_1) q_2$, and $\beta_o =
(1-q_1)(1-q_2)q(1-q)^{o-3}$ for $o\ge 3$.

The likelihood can be computed using similar formula to the simplified scenario:
$$L(\theta | H_k) = \sum_{j=1}^m H_k[j] \log p_j.$$
where $\theta = \{c_k, q_1, q_2, q\}$ is the parameter vector.

\subsection{Modeling Errors}

In~\cite{waterman} and~\cite{williams} they handled the sequencing errors very simply. In~\cite{waterman} they did not consider the errors in the data at all and run their program on error corrected data. This might add some biases from the error correcting program, cause loss of part of data and it also increases the time complexity of the process. In~\cite{williams} they filtered errors simply by truncating the low abundance part of the histogram. This requires higher coverage.

In our paper\cite{covest}, we focused on modeling errors directly. We provided several probabilistic models graded by complexity.
The simplest model considered errors contributing only to abundance of one in the histogram (i.e.\ every error creates a unique $k$-mer) and does not consider any repeats. The model is a mixture of two distributions --- the distribution for erroneous \kmers and the distribution for error free \kmers. The abundances of the error free \kmers are generated from the truncated Poisson distribution with mean $c'$. Let $\epsilon$ be the error rate, we can compute $c_k$ from $c'$ by dividing it by the probability that \kmer contains no error: $c_k = c'/{(1-\epsilon)}^k$.

The probability that the same error happen multiple times increases with increasing coverage. Hence the second model considers errors contributing also to higher abundances.
The second model is a mixture of truncated Poisson distributions with means $\lambda_s \forall s \in \{0\dots k\}$, where $s$ is a number of errors in each \kmer considered in the partial distribution. This model still does not consider any repeats in the original sequence.
This is fixed in final model, where repeats were modeled by geometric distribution (see section~\ref{subsec:repeatmodles}).

\todo{short conclusion}

\subsection{Testing the Models}

\todo{ako sa testuju}
\todo{sumar z vysledkov clankov}

\section{Conclusion}

\todo{toto este treba upravit}

In this chapter we dealt with a problem of estimating the genome size and the coverage. We showed two main approaches to this problems based on \kmer histogram analysis.

The first approach was based on precise modeling of repeat content and omitted the error modeling, whereas the second approach was based on precise error modeling and simple repeat modeling.
The first approach works well on high coverage data and is more suitable for genomes, where the repeat content is very different from the simple model in the second approach.

The second approach works well even in very low coverage data ($0.5\times$) or with highly erroneous data ($10\%$ error rate). This approach is very universal and easily extensible to more difficult models.

The possibility of estimating the genome size and the coverage from low coverage data is very useful for cheaper experiment design and for analysis of large genomes, such as plant genomes where the sizes are in tens of gigabases.
